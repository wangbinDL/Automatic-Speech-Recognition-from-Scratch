SubwordTokenizer(./spm/lrs2/1000_bpe.model, case=upper, vocab=1000)
ASRDataset: 45839 samples, 734 mini-batches. batch_size: 64, batch_seconds: 256, mean batch-size: 62.45
Transformer(
  (frontend): ResNet1D(
    (conv2d): Sequential(
      (0): Conv2d(1, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=(0, 0), dilation=1, ceil_mode=False)
    )
    (layer1): Sequential(
      (0): BasicBlock1D(
        (conv1): Conv1d(80, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv1d(80, 64, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock1D(
        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock1D(
        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock1D(
        (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock1D(
        (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock1D(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock1D(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv1d(256, 256, kernel_size=(1,), stride=(2,), bias=False)
          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock1D(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (encoder): Encoder(
    (pos_emb): Embedding(2048, 256)
    (emb_dropout): Dropout(p=0.1, inplace=False)
    (layers): ModuleList(
      (0-11): 12 x EncoderLayer(
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (multi_head_attn): MultiHeadAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (wq): Linear(in_features=256, out_features=256, bias=True)
          (wk): Linear(in_features=256, out_features=256, bias=True)
          (wv): Linear(in_features=256, out_features=256, bias=True)
          (W_out): Linear(in_features=256, out_features=256, bias=True)
        )
        (poswise_ffn): PoswiseFFN(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (relu): ReLU(inplace=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (decoder): Decoder(
    (tgt_emb): Embedding(1000, 256)
    (dropout_emb): Dropout(p=0.1, inplace=False)
    (pos_emb): Embedding(2048, 256)
    (layers): ModuleList(
      (0-5): 6 x DecoderLayer(
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (poswise_ffn): PoswiseFFN(
          (lin1): Linear(in_features=256, out_features=2048, bias=True)
          (lin2): Linear(in_features=2048, out_features=256, bias=True)
          (relu): ReLU(inplace=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (dec_attn): MultiHeadAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (wq): Linear(in_features=256, out_features=256, bias=True)
          (wk): Linear(in_features=256, out_features=256, bias=True)
          (wv): Linear(in_features=256, out_features=256, bias=True)
          (W_out): Linear(in_features=256, out_features=256, bias=True)
        )
        (enc_dec_attn): MultiHeadAttention(
          (dropout): Dropout(p=0.0, inplace=False)
          (wq): Linear(in_features=256, out_features=256, bias=True)
          (wk): Linear(in_features=256, out_features=256, bias=True)
          (wv): Linear(in_features=256, out_features=256, bias=True)
          (W_out): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (linear): Linear(in_features=256, out_features=1000, bias=True)
)
Epoch: 01/50, Loss: 8.40
Epoch: 02/50, Loss: 6.08
Epoch: 03/50, Loss: 5.75
Epoch: 04/50, Loss: 5.45
Epoch: 05/50, Loss: 5.12
Epoch: 06/50, Loss: 4.82
Epoch: 07/50, Loss: 4.51
Epoch: 08/50, Loss: 4.20
Epoch: 09/50, Loss: 3.85
Epoch: 10/50, Loss: 3.51
Epoch: 11/50, Loss: 3.16
Epoch: 12/50, Loss: 2.85
Epoch: 13/50, Loss: 2.58
Epoch: 14/50, Loss: 2.34
Epoch: 15/50, Loss: 2.16
Epoch: 16/50, Loss: 2.00
Epoch: 17/50, Loss: 1.88
Epoch: 18/50, Loss: 1.77
Epoch: 19/50, Loss: 1.69
Epoch: 20/50, Loss: 1.61
Epoch: 21/50, Loss: 1.54
Epoch: 22/50, Loss: 1.48
Epoch: 23/50, Loss: 1.42
Epoch: 24/50, Loss: 1.38
Epoch: 25/50, Loss: 1.34
Epoch: 26/50, Loss: 1.31
Epoch: 27/50, Loss: 1.28
Epoch: 28/50, Loss: 1.25
Epoch: 29/50, Loss: 1.23
Epoch: 30/50, Loss: 1.21
Epoch: 31/50, Loss: 1.19
Epoch: 32/50, Loss: 1.18
Epoch: 33/50, Loss: 1.16
Epoch: 34/50, Loss: 1.15
Epoch: 35/50, Loss: 1.14
Epoch: 36/50, Loss: 1.13
Epoch: 37/50, Loss: 1.12
Epoch: 38/50, Loss: 1.11
Epoch: 39/50, Loss: 1.11
Epoch: 40/50, Loss: 1.10
Epoch: 41/50, Loss: 1.10
Epoch: 42/50, Loss: 1.09
Epoch: 43/50, Loss: 1.09
Epoch: 44/50, Loss: 1.08
Epoch: 45/50, Loss: 1.08
Epoch: 46/50, Loss: 1.08
Epoch: 47/50, Loss: 1.08
Epoch: 48/50, Loss: 1.08
Epoch: 49/50, Loss: 1.08
Epoch: 50/50, Loss: 1.08
